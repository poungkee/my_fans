"""
FANS í¸í–¥ì„± ë¶„ì„ AI í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
- AI-Hub ë¼ë²¨ë§ ë°ì´í„°ë¥¼ ì‚¬ìš©í•œ ë¬¸ì¥ ìœ í˜• ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ
- ì „ì²´ ë°ì´í„°ì…‹ ì‚¬ìš© (166,339 ë¬¸ì¥)
"""

import os
import json
import pickle
import glob
from collections import Counter
from datetime import datetime
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, accuracy_score
import warnings
warnings.filterwarnings('ignore')

# ê²½ë¡œ ì„¤ì •
BASE_DIR = "/Users/hodduk/Documents/git/FANS/backend/ai/bias-analysis-ai"
TRAIN_DATA_DIR = os.path.join(BASE_DIR, "ai-training/159.ë¬¸ì¥ ìœ í˜•(ì¶”ë¡ , ì˜ˆì¸¡ ë“±) íŒë‹¨ ë°ì´í„°/01-1.ì •ì‹ê°œë°©ë°ì´í„°/Training/02.ë¼ë²¨ë§ë°ì´í„°")
VALID_DATA_DIR = os.path.join(BASE_DIR, "ai-training/159.ë¬¸ì¥ ìœ í˜•(ì¶”ë¡ , ì˜ˆì¸¡ ë“±) íŒë‹¨ ë°ì´í„°/01-1.ì •ì‹ê°œë°©ë°ì´í„°/Validation/02.ë¼ë²¨ë§ë°ì´í„°")
MODEL_DIR = os.path.join(BASE_DIR, "models")

# ëª¨ë¸ ë””ë ‰í† ë¦¬ ìƒì„±
os.makedirs(MODEL_DIR, exist_ok=True)

class BiasAnalysisTrainer:
    def __init__(self):
        self.data = []
        self.vectorizer = TfidfVectorizer(max_features=10000, min_df=2, max_df=0.95)
        self.model = None

    def load_json_data(self, data_dir):
        """JSON ë¼ë²¨ë§ ë°ì´í„° ë¡œë“œ"""
        json_files = glob.glob(os.path.join(data_dir, "**/*.json"), recursive=True)

        sentences = []
        for json_file in json_files:
            try:
                with open(json_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)

                    # annotation ë°°ì—´ì—ì„œ ë¬¸ì¥ê³¼ ë¼ë²¨ ì¶”ì¶œ
                    if 'annotation' in data:
                        for item in data['annotation']:
                            # í•µì‹¬ë™ì‚¬ê°€ ì•„ë‹Œ ë¬¸ì¥ ìœ í˜•ë§Œ ì¶”ì¶œ
                            if item.get('label') in ['ì‚¬ì‹¤í˜•', 'ì¶”ë¡ í˜•', 'ì˜ˆì¸¡í˜•', 'ëŒ€í™”í˜•']:
                                sentences.append({
                                    'text': item['text'],
                                    'label': item['label']
                                })
            except Exception as e:
                print(f"íŒŒì¼ ì½ê¸° ì‹¤íŒ¨ {json_file}: {e}")
                continue

        return sentences

    def load_data(self):
        """í•™ìŠµ ë°ì´í„° ë¡œë“œ"""
        print("=" * 50)
        print("1. ë°ì´í„° ë¡œë”© ì‹œì‘...")
        print("=" * 50)

        # Training ë°ì´í„° ë¡œë“œ
        print(f"Training ë°ì´í„° ë¡œë”©: {TRAIN_DATA_DIR}")
        train_sentences = self.load_json_data(TRAIN_DATA_DIR)
        print(f"Training ë¬¸ì¥ ìˆ˜: {len(train_sentences)}")

        # Validation ë°ì´í„°ë„ í•™ìŠµì— í¬í•¨
        print(f"\nValidation ë°ì´í„° ë¡œë”©: {VALID_DATA_DIR}")
        valid_sentences = self.load_json_data(VALID_DATA_DIR)
        print(f"Validation ë¬¸ì¥ ìˆ˜: {len(valid_sentences)}")

        # ì „ì²´ ë°ì´í„° í•©ì¹˜ê¸°
        self.data = train_sentences + valid_sentences
        print(f"\nì´ í•™ìŠµ ë°ì´í„° ìˆ˜: {len(self.data)}")

        # ë¼ë²¨ ë¶„í¬ í™•ì¸
        labels = [d['label'] for d in self.data]
        label_dist = Counter(labels)
        print("\në¼ë²¨ ë¶„í¬:")
        for label, count in sorted(label_dist.items()):
            print(f"  {label}: {count:,} ({count/len(labels)*100:.1f}%)")

    def prepare_features(self):
        """íŠ¹ì§• ì¶”ì¶œ"""
        print("\n" + "=" * 50)
        print("2. íŠ¹ì§• ì¶”ì¶œ ì¤‘...")
        print("=" * 50)

        texts = [d['text'] for d in self.data]
        labels = [d['label'] for d in self.data]

        X = self.vectorizer.fit_transform(texts)
        y = np.array(labels)

        print(f"íŠ¹ì§• ì°¨ì›: {X.shape}")

        return X, y

    def train_models(self, X, y):
        """ì—¬ëŸ¬ ëª¨ë¸ í•™ìŠµ ë° ë¹„êµ"""
        print("\n" + "=" * 50)
        print("3. ëª¨ë¸ í•™ìŠµ ì‹œì‘...")
        print("=" * 50)

        # í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë¶„í• 
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )

        print(f"í•™ìŠµ ë°ì´í„°: {X_train.shape[0]:,}")
        print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„°: {X_test.shape[0]:,}")

        # ëª¨ë¸ ì •ì˜
        models = {
            'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),
            'Random Forest': RandomForestClassifier(n_estimators=200, max_depth=50, random_state=42, n_jobs=-1),
            'Naive Bayes': MultinomialNB()
        }

        best_model = None
        best_score = 0
        best_name = ""

        print("\nëª¨ë¸ë³„ ì„±ëŠ¥ ë¹„êµ:")
        print("-" * 40)

        for name, model in models.items():
            print(f"\n{name} í•™ìŠµ ì¤‘...")
            model.fit(X_train, y_train)
            y_pred = model.predict(X_test)
            score = accuracy_score(y_test, y_pred)
            print(f"ì •í™•ë„: {score:.4f}")

            if score > best_score:
                best_score = score
                best_model = model
                best_name = name

        print(f"\nìµœê³  ëª¨ë¸: {best_name} (ì •í™•ë„: {best_score:.4f})")

        self.model = best_model
        return X_test, y_test

    def evaluate(self, X_test, y_test):
        """ëª¨ë¸ í‰ê°€"""
        print("\n" + "=" * 50)
        print("4. ìƒì„¸ í‰ê°€ ê²°ê³¼")
        print("=" * 50)

        y_pred = self.model.predict(X_test)
        print(classification_report(y_test, y_pred))

    def save_model(self):
        """ëª¨ë¸ ì €ì¥"""
        print("\n" + "=" * 50)
        print("5. ëª¨ë¸ ì €ì¥ ì¤‘...")
        print("=" * 50)

        # ëª¨ë¸ ì €ì¥
        model_path = os.path.join(MODEL_DIR, "bias_model.pkl")
        with open(model_path, 'wb') as f:
            pickle.dump(self.model, f)
        print(f"âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {model_path}")

        # ë²¡í„°ë¼ì´ì € ì €ì¥
        vectorizer_path = os.path.join(MODEL_DIR, "vectorizer.pkl")
        with open(vectorizer_path, 'wb') as f:
            pickle.dump(self.vectorizer, f)
        print(f"âœ… ë²¡í„°ë¼ì´ì € ì €ì¥ ì™„ë£Œ: {vectorizer_path}")

        # ë©”íƒ€ë°ì´í„° ì €ì¥
        metadata = {
            'model_type': type(self.model).__name__,
            'trained_at': datetime.now().isoformat(),
            'n_samples': len(self.data),
            'labels': list(set([d['label'] for d in self.data]))
        }
        metadata_path = os.path.join(MODEL_DIR, "metadata.json")
        with open(metadata_path, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)
        print(f"âœ… ë©”íƒ€ë°ì´í„° ì €ì¥ ì™„ë£Œ: {metadata_path}")

    def test_model(self):
        """ëª¨ë¸ í…ŒìŠ¤íŠ¸"""
        print("\n" + "=" * 50)
        print("6. ëª¨ë¸ í…ŒìŠ¤íŠ¸")
        print("=" * 50)

        test_sentences = [
            "ì •ë¶€ëŠ” ì˜¤ëŠ˜ ìƒˆë¡œìš´ ê²½ì œì •ì±…ì„ ë°œí‘œí–ˆë‹¤.",
            "ì „ë¬¸ê°€ë“¤ì€ ë‚´ë…„ ê²½ì œê°€ íšŒë³µë  ê²ƒìœ¼ë¡œ ì˜ˆì¸¡í•œë‹¤.",
            "ì´ë²ˆ ì •ì±…ì´ ì„±ê³µí•˜ë ¤ë©´ êµ­ë¯¼ì˜ í˜‘ì¡°ê°€ í•„ìš”í•˜ë‹¤ê³  ê°•ì¡°í–ˆë‹¤.",
            "ê¹€ ì˜ì›ì€ 'ì •ë¶€ ì •ì±…ì— ë¬¸ì œê°€ ìˆë‹¤'ê³  ë§í–ˆë‹¤.",
            "ì½”ìŠ¤í”¼ ì§€ìˆ˜ê°€ 2% ìƒìŠ¹í–ˆë‹¤.",
            "ë¶„ì„ê°€ë“¤ì€ ì£¼ê°€ê°€ ë” ì˜¤ë¥¼ ê°€ëŠ¥ì„±ì´ ìˆë‹¤ê³  ë³¸ë‹¤."
        ]

        print("\ní…ŒìŠ¤íŠ¸ ë¬¸ì¥ ë¶„ë¥˜ ê²°ê³¼:")
        print("-" * 60)

        for sent in test_sentences:
            X_test = self.vectorizer.transform([sent])
            pred = self.model.predict(X_test)[0]
            proba = self.model.predict_proba(X_test)[0].max()
            print(f"\në¬¸ì¥: {sent}")
            print(f"ë¶„ë¥˜: {pred} (í™•ë¥ : {proba:.2f})")

def main():
    start_time = datetime.now()

    print("ğŸš€ " * 20)
    print("FANS í¸í–¥ì„± ë¶„ì„ AI í•™ìŠµ ì‹œì‘")
    print("ğŸš€ " * 20)

    # í•™ìŠµ ì‹¤í–‰
    trainer = BiasAnalysisTrainer()
    trainer.load_data()
    X, y = trainer.prepare_features()
    X_test, y_test = trainer.train_models(X, y)
    trainer.evaluate(X_test, y_test)
    trainer.save_model()
    trainer.test_model()

    # ì™„ë£Œ
    elapsed = datetime.now() - start_time
    print("\n" + "=" * 50)
    print("âœ¨ í•™ìŠµ ì™„ë£Œ!")
    print("=" * 50)
    print(f"ì´ í•™ìŠµ ì‹œê°„: {elapsed}")
    print(f"í•™ìŠµëœ ëª¨ë¸ ìœ„ì¹˜: {MODEL_DIR}")
    print("\nì´ì œ í•™ìŠµëœ ëª¨ë¸ì„ APIì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main()