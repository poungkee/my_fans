"""
ë‰´ìŠ¤ ë¶„ë¥˜ íŒŒì´í”„ë¼ì¸ DAG
Kafka -> Spark ML ë¶„ë¥˜ -> DB ì €ìž¥
"""

from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.bash import BashOperator
from datetime import datetime, timedelta
import sys
import os

# Spark ê²½ë¡œ ì¶”ê°€
sys.path.insert(0, '/opt/airflow/recommendation')

default_args = {
    'owner': 'fans',
    'depends_on_past': False,
    'start_date': datetime(2025, 10, 14),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

dag = DAG(
    'news_classification_pipeline',
    default_args=default_args,
    description='ë‰´ìŠ¤ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ íŒŒì´í”„ë¼ì¸',
    schedule_interval='*/10 * * * *',  # 10ë¶„ë§ˆë‹¤ ì‹¤í–‰
    catchup=False,
    tags=['news', 'classification', 'spark'],
)

def train_classification_model():
    """Spark ML ëª¨ë¸ í•™ìŠµ"""
    from pyspark.sql import SparkSession
    from category_classifier import CategoryClassifier

    print("ðŸš€ Spark ì„¸ì…˜ ì‹œìž‘...")
    spark = SparkSession.builder \
        .appName("CategoryClassifierTraining") \
        .master("spark://spark-master:7077") \
        .config("spark.executor.memory", "1g") \
        .getOrCreate()

    try:
        print("ðŸŽ“ ëª¨ë¸ í•™ìŠµ ì‹œìž‘...")
        classifier = CategoryClassifier(spark)
        classifier.train_model()
        print("âœ… ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!")
        return "success"
    except Exception as e:
        print(f"âŒ í•™ìŠµ ì‹¤íŒ¨: {e}")
        raise
    finally:
        spark.stop()

def classify_news_from_kafka():
    """Kafkaì—ì„œ ë‰´ìŠ¤ ì½ì–´ì„œ ë¶„ë¥˜"""
    from pyspark.sql import SparkSession
    from pyspark.sql.functions import from_json, col
    from pyspark.sql.types import StructType, StructField, StringType, IntegerType
    from category_classifier import CategoryClassifier
    import psycopg2

    print("ðŸš€ Spark Streaming ì‹œìž‘...")
    spark = SparkSession.builder \
        .appName("NewsClassification") \
        .master("spark://spark-master:7077") \
        .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0") \
        .config("spark.executor.memory", "1g") \
        .getOrCreate()

    try:
        # Kafkaì—ì„œ ì½ê¸°
        schema = StructType([
            StructField("id", IntegerType(), True),
            StructField("title", StringType(), True),
            StructField("content", StringType(), True),
            StructField("url", StringType(), True)
        ])

        df = spark \
            .read \
            .format("kafka") \
            .option("kafka.bootstrap.servers", "kafka:29092") \
            .option("subscribe", "news-raw") \
            .option("startingOffsets", "earliest") \
            .load()

        # JSON íŒŒì‹±
        news_df = df.select(
            from_json(col("value").cast("string"), schema).alias("data")
        ).select("data.*")

        if news_df.count() == 0:
            print("ðŸ“­ ì²˜ë¦¬í•  ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return "no_data"

        print(f"ðŸ“° {news_df.count()}ê°œ ë‰´ìŠ¤ ì²˜ë¦¬ ì¤‘...")

        # ë¶„ë¥˜ê¸° ë¡œë“œ
        classifier = CategoryClassifier(spark)
        if not classifier.load_model():
            print("ðŸŽ“ ëª¨ë¸ì´ ì—†ì–´ì„œ í•™ìŠµí•©ë‹ˆë‹¤...")
            classifier.train_model()

        # DB ì—°ê²°
        conn = psycopg2.connect(
            host="postgres",
            database=os.getenv("POSTGRES_DB", "fans_db"),
            user=os.getenv("POSTGRES_USER", "fans_user"),
            password=os.getenv("POSTGRES_PASSWORD", "fans_password")
        )
        cursor = conn.cursor()

        # ê° ë‰´ìŠ¤ ë¶„ë¥˜
        news_list = news_df.collect()
        classified_count = 0

        for news in news_list:
            try:
                # ë¶„ë¥˜
                category_name, category_id, confidence = classifier.predict(
                    news.title,
                    news.content
                )

                # DB ì—…ë°ì´íŠ¸
                cursor.execute("""
                    UPDATE news_articles
                    SET category_id = %s
                    WHERE id = %s
                """, (category_id, news.id))

                classified_count += 1
                print(f"âœ… ID {news.id}: {category_name} (ì‹ ë¢°ë„: {confidence:.2f})")

            except Exception as e:
                print(f"âŒ ID {news.id} ë¶„ë¥˜ ì‹¤íŒ¨: {e}")
                continue

        conn.commit()
        cursor.close()
        conn.close()

        print(f"ðŸŽ‰ {classified_count}/{len(news_list)}ê°œ ë‰´ìŠ¤ ë¶„ë¥˜ ì™„ë£Œ!")
        return f"classified_{classified_count}"

    except Exception as e:
        print(f"âŒ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        raise
    finally:
        spark.stop()

# Task ì •ì˜
train_model_task = PythonOperator(
    task_id='train_classification_model',
    python_callable=train_classification_model,
    dag=dag,
)

classify_task = PythonOperator(
    task_id='classify_news',
    python_callable=classify_news_from_kafka,
    dag=dag,
)

# Task ìˆœì„œ
train_model_task >> classify_task
