import 'reflect-metadata';
import dotenv from 'dotenv';

// í™˜ê²½ë³€ìˆ˜ ë¨¼ì € ë¡œë“œ (docker-composeì˜ env_fileì´ ìš°ì„ , ì—†ìœ¼ë©´ ë¡œì»¬ .env)
dotenv.config();

import express from 'express';
import cors from 'cors';
import { AppDataSource } from '../shared/config/database';
import { newsCrawlerService } from './services/newsCrawlerService';
import { schedulerService } from './services/schedulerService';
import logger from '../shared/config/logger';

const app = express();
const PORT = parseInt(process.env.API_CRAWLER_PORT || '4003', 10);

app.use(cors());
app.use(express.json());

// í—¬ìŠ¤ì²´í¬
app.get('/health', (req, res) => {
  res.json({
    status: 'ok',
    timestamp: new Date().toISOString(),
    service: 'FANS API Crawler Service'
  });
});

// API í¬ë¡¤ë§ ì‹œì‘
app.post('/crawl/start', async (req, res) => {
  try {
    logger.info('Request body:', req.body);
    const limit = Number(req.body?.limit) || 5; // ê¸°ë³¸ê°’ 5
    logger.info(`ğŸ“° API í¬ë¡¤ë§ ì‹œì‘... (ì¹´í…Œê³ ë¦¬ë‹¹ ${limit}ê°œ)`);
    const results = await newsCrawlerService.crawlAllCategories(limit);

    let totalCollected = 0;
    const summary: string[] = [];

    for (const [category, articles] of Object.entries(results)) {
      totalCollected += articles.length;
      summary.push(`${category}: ${articles.length}ê°œ`);
    }

    res.json({
      message: 'API í¬ë¡¤ë§ ì™„ë£Œ',
      totalCollected,
      results: summary,
      timestamp: new Date().toISOString()
    });
  } catch (error) {
    logger.error('API í¬ë¡¤ë§ ì‹¤íŒ¨:', error);
    res.status(500).json({ error: 'API í¬ë¡¤ë§ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤' });
  }
});

// ì§€ì›í•˜ëŠ” ì¹´í…Œê³ ë¦¬ ì¡°íšŒ
app.get('/categories', (req, res) => {
  res.json({
    supportedCategories: newsCrawlerService.getSupportedCategories(),
    timestamp: new Date().toISOString()
  });
});

// ê¸°ì¡´ ê¸°ì‚¬ í¸í–¥ì„± ë¶„ì„ (ë¶„ì„ ì•ˆëœ ê¸°ì‚¬ë“¤)
app.post('/analyze/backfill', async (req, res) => {
  try {
    const limit = Number(req.body?.limit) || 100; // ê¸°ë³¸ê°’ 100ê°œì”©
    logger.info(`ğŸ“Š ê¸°ì¡´ ê¸°ì‚¬ í¸í–¥ì„± ë¶„ì„ ì‹œì‘... (ìµœëŒ€ ${limit}ê°œ)`);

    const result = await newsCrawlerService.analyzeExistingArticles(limit);

    res.json({
      message: 'ê¸°ì¡´ ê¸°ì‚¬ ë¶„ì„ ì™„ë£Œ',
      ...result,
      timestamp: new Date().toISOString()
    });
  } catch (error) {
    logger.error('ê¸°ì¡´ ê¸°ì‚¬ ë¶„ì„ ì‹¤íŒ¨:', error);
    res.status(500).json({ error: 'ê¸°ì¡´ ê¸°ì‚¬ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤' });
  }
});

// í¬ë¡¤ëŸ¬ ìƒíƒœ ì¡°íšŒ
app.get('/status', (_req, res) => {
  const schedulerStatus = schedulerService.getStatus();
  res.json({
    status: 'running',
    scheduler: schedulerStatus,
    supportedCategories: newsCrawlerService.getSupportedCategories(),
    endpoints: [
      'POST /crawl/start - API í¬ë¡¤ë§ ì‹œì‘',
      'POST /analyze/backfill - ê¸°ì¡´ ê¸°ì‚¬ í¸í–¥ì„± ë¶„ì„',
      'POST /scheduler/start - ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘',
      'POST /scheduler/stop - ìŠ¤ì¼€ì¤„ëŸ¬ ì¤‘ì§€',
      'GET /categories - ì§€ì›í•˜ëŠ” ì¹´í…Œê³ ë¦¬ ëª©ë¡',
      'GET /status - í¬ë¡¤ëŸ¬ ìƒíƒœ ì¡°íšŒ',
      'GET /health - í—¬ìŠ¤ì²´í¬'
    ],
    timestamp: new Date().toISOString()
  });
});

// ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘
app.post('/scheduler/start', (req, res) => {
  try {
    const config = req.body;
    schedulerService.start(config);
    res.json({
      message: 'ìŠ¤ì¼€ì¤„ëŸ¬ê°€ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤',
      status: schedulerService.getStatus()
    });
  } catch (error: any) {
    logger.error('ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘ ì‹¤íŒ¨:', error);
    res.status(500).json({ error: error?.message || 'ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘ ì‹¤íŒ¨' });
  }
});

// ìŠ¤ì¼€ì¤„ëŸ¬ ì¤‘ì§€
app.post('/scheduler/stop', (_req, res) => {
  try {
    schedulerService.stop();
    res.json({
      message: 'ìŠ¤ì¼€ì¤„ëŸ¬ê°€ ì¤‘ì§€ë˜ì—ˆìŠµë‹ˆë‹¤',
      status: schedulerService.getStatus()
    });
  } catch (error: any) {
    logger.error('ìŠ¤ì¼€ì¤„ëŸ¬ ì¤‘ì§€ ì‹¤íŒ¨:', error);
    res.status(500).json({ error: error?.message || 'ìŠ¤ì¼€ì¤„ëŸ¬ ì¤‘ì§€ ì‹¤íŒ¨' });
  }
});

async function startServer() {
  try {
    await AppDataSource.initialize();
    logger.info('âœ… Database connected successfully');

    app.listen(PORT, '0.0.0.0', () => {
      logger.info(`ğŸš€ API Crawler Service running on port ${PORT}`);
      logger.info(`ğŸ“Š Health check: http://localhost:${PORT}/health`);
      logger.info(`ğŸ“° API crawl: POST http://localhost:${PORT}/crawl/start`);
      logger.info(`ğŸ“‹ API categories: GET http://localhost:${PORT}/categories`);
      logger.info(`ğŸ•’ Scheduler status: GET http://localhost:${PORT}/status`);

      // ìë™ í¬ë¡¤ë§ í™œì„±í™” (í™˜ê²½ë³€ìˆ˜ë¡œ ì œì–´)
      const autoStart = process.env.AUTO_CRAWL === 'true';
      if (autoStart) {
        const intervalMinutes = parseInt(process.env.CRAWL_INTERVAL_MINUTES || '5');
        const limitPerCategory = parseInt(process.env.CRAWL_LIMIT_PER_CATEGORY || '5');

        logger.info(`\nâ° ìë™ í¬ë¡¤ë§ í™œì„±í™” - ${intervalMinutes}ë¶„ë§ˆë‹¤ ì‹¤í–‰ (ì¹´í…Œê³ ë¦¬ë‹¹ ${limitPerCategory}ê°œ)`);
        schedulerService.start({
          intervalMinutes,
          limitPerCategory,
          enabled: true
        });
      } else {
        logger.info('\nâ¸ï¸  ìë™ í¬ë¡¤ë§ ë¹„í™œì„±í™” - ìˆ˜ë™ ì‹¤í–‰ ëª¨ë“œ');
        logger.info('   ì‹œì‘í•˜ë ¤ë©´: POST http://localhost:${PORT}/scheduler/start');
      }
    });
  } catch (error) {
    logger.error('âŒ Failed to start API crawler service:', error);
    process.exit(1);
  }
}

startServer();