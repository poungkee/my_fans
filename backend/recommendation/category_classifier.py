"""
âš ï¸ ì´ íŒŒì¼ì€ ë” ì´ìƒ ì‚¬ìš©ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤ âš ï¸

Spark ML ë¶„ë¥˜ê¸° â†’ Simple Classifier APIë¡œ ëŒ€ì²´ë¨

ì‹¤ì œë¡œëŠ” ì‚¬ìš©ë˜ì§€ ì•Šì•˜ë˜ ì½”ë“œì…ë‹ˆë‹¤.
classification_api.pyë„ ì´ ë¶„ë¥˜ê¸°ë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šê³ 
ì›ë³¸ ì¹´í…Œê³ ë¦¬ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ê³  ìˆì—ˆìŠµë‹ˆë‹¤.

ì•„ë˜ ì½”ë“œëŠ” ì°¸ê³ ìš©ìœ¼ë¡œ ì£¼ì„ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤.
í•„ìš” ì‹œ ë³µêµ¬ ê°€ëŠ¥í•˜ë„ë¡ ì½”ë“œë¥¼ ì‚­ì œí•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.
"""

'''
# ===== ì•„ë˜ ì½”ë“œëŠ” ì£¼ì„ ì²˜ë¦¬ë¨ (Spark ëŒ€ì²´) =====

"""
Spark ML ê¸°ë°˜ ë‰´ìŠ¤ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ê¸°
í•œêµ­ì–´ ë‰´ìŠ¤ë¥¼ 8ê°œ ì¹´í…Œê³ ë¦¬ë¡œ ë¶„ë¥˜
"""

from pyspark.sql import SparkSession
from pyspark.ml.feature import HashingTF, IDF, Tokenizer, StopWordsRemover
from pyspark.ml.classification import NaiveBayes, NaiveBayesModel
from pyspark.ml import Pipeline, PipelineModel
from pyspark.sql.types import StructType, StructField, StringType, IntegerType
from pyspark.sql.functions import udf, col
import re
import os

class CategoryClassifier:
    """Spark MLì„ ì‚¬ìš©í•œ ë‰´ìŠ¤ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ê¸°"""

    def __init__(self, spark: SparkSession):
        self.spark = spark
        self.categories = {
            0: 'ê¸°íƒ€',
            1: 'ì •ì¹˜',
            2: 'ê²½ì œ',
            3: 'ì‚¬íšŒ',
            4: 'ìƒí™œ/ë¬¸í™”',
            5: 'IT/ê³¼í•™',
            6: 'ì„¸ê³„',
            7: 'ìŠ¤í¬ì¸ ',
            8: 'ì—°ì˜ˆ'
        }

        self.category_to_id = {v: k for k, v in self.categories.items()}

        # í•œêµ­ì–´ ë¶ˆìš©ì–´ (ê°„ë‹¨í•œ ë²„ì „)
        self.korean_stopwords = [
            'ì´', 'ê·¸', 'ì €', 'ê²ƒ', 'ìˆ˜', 'ë“±', 'ë“¤', 'ë°', 'ë“±', 'ë…„', 'ì›”', 'ì¼',
            'í–ˆë‹¤', 'ìˆë‹¤', 'ì´ë‹¤', 'ë˜ë‹¤', 'í•˜ë‹¤', 'ì´', 'ê°€', 'ì„', 'ë¥¼', 'ì—', 'ì˜',
            'í•œ', 'ì€', 'ëŠ”', 'ë„', 'ì™€', 'ê³¼', 'ë¡œ', 'ìœ¼ë¡œ', 'ìœ¼', 'ì´'
        ]

        self.model_path = "/app/models/category_classifier"
        self.pipeline_model = None

    @staticmethod
    def preprocess_text(text):
        """í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬"""
        if not text:
            return ""

        # íŠ¹ìˆ˜ë¬¸ì ì œê±° (í•œê¸€, ì˜ë¬¸, ìˆ«ìë§Œ ìœ ì§€)
        text = re.sub(r'[^ê°€-í£a-zA-Z0-9\s]', ' ', text)
        # ì—¬ëŸ¬ ê³µë°±ì„ í•˜ë‚˜ë¡œ
        text = re.sub(r'\s+', ' ', text)
        return text.strip().lower()

    def create_training_data(self):
        """í•™ìŠµìš© ë°ì´í„° ìƒì„± (í‚¤ì›Œë“œ ê¸°ë°˜)"""

        # ì¹´í…Œê³ ë¦¬ë³„ ëŒ€í‘œ í‚¤ì›Œë“œ ìƒ˜í”Œ
        training_samples = [
            # ì •ì¹˜
            ("êµ­íšŒ ì˜ì› ë²•ì•ˆ ì •ë¶€ ëŒ€í†µë ¹ ì—¬ë‹¹ ì•¼ë‹¹ ì •ì±…", 1),
            ("ì„ ê±° íˆ¬í‘œ ê³µì•½ ì •ë‹¹ êµ­ë¬´ì´ë¦¬ ì¥ê´€", 1),
            ("ì •ì¹˜ê¶Œ ì—¬ì•¼ êµ­ì •ê°ì‚¬ ì²­ì™€ëŒ€ ê°œê°", 1),
            ("ì˜ì • í–‰ì • ì™¸êµ í†µì¼ ì•ˆë³´ êµ­ë°©ë¶€", 1),

            # ê²½ì œ
            ("ê¸°ì—… ê²½ì œ ì‹œì¥ ì£¼ì‹ ì¦ê¶Œ ê¸ˆìœµ ì€í–‰", 2),
            ("íˆ¬ì ìˆ˜ì¶œ ìˆ˜ì… ë¬´ì—­ í™˜ìœ¨ ê¸ˆë¦¬ ë¬¼ê°€", 2),
            ("ë¶€ë™ì‚° ì½”ìŠ¤í”¼ ì½”ìŠ¤ë‹¥ ë°˜ë„ì²´ ìë™ì°¨", 2),
            ("ì‚¼ì„± í˜„ëŒ€ LG SK ë§¤ì¶œ ì˜ì—…ì´ìµ ì‹¤ì ", 2),
            ("ê²½ì˜ CEO ì¬ê³„ ì˜ˆì‚° ì„¸ê¸ˆ ì¬ë¬´ë¶€", 2),

            # ì‚¬íšŒ
            ("ì‚¬ê±´ ì‚¬ê³  ë²”ì£„ ê²½ì°° ê²€ì°° ë²•ì› íŒê²°", 3),
            ("êµìœ¡ í•™êµ ëŒ€í•™ í•™ìƒ êµì‚¬ ì…ì‹œ ìˆ˜ëŠ¥", 3),
            ("í™˜ê²½ ê¸°í›„ ë¯¸ì„¸ë¨¼ì§€ ë³µì§€ ì˜ë£Œ ë³‘ì›", 3),
            ("ê³ ìš© ì‹¤ì—… ë…¸ë™ ì„ê¸ˆ ìµœì €ì„ê¸ˆ ì•„íŒŒíŠ¸", 3),

            # ìƒí™œ/ë¬¸í™”
            ("ë¬¸í™” ì˜ˆìˆ  ê³µì—° ì „ì‹œ ì˜í™” ë“œë¼ë§ˆ ìŒì•…", 4),
            ("ì—¬í–‰ ê´€ê´‘ ë§›ì§‘ ìŒì‹ íŒ¨ì…˜ ë·°í‹° í™”ì¥í’ˆ", 4),
            ("ê±´ê°• ìš´ë™ ë‹¤ì´ì–´íŠ¸ ìœ¡ì•„ ê²°í˜¼ ê°€ì¡±", 4),
            ("ë°˜ë ¤ë™ë¬¼ ê°•ì•„ì§€ ê³ ì–‘ì´ ì±… ë„ì„œ ì†Œì„¤", 4),

            # IT/ê³¼í•™
            ("IT ê³¼í•™ ê¸°ìˆ  ì»´í“¨í„° ì†Œí”„íŠ¸ì›¨ì–´ ì¸í„°ë„·", 5),
            ("AI ì¸ê³µì§€ëŠ¥ ë¨¸ì‹ ëŸ¬ë‹ ë¡œë´‡ ìŠ¤ë§ˆíŠ¸í°", 5),
            ("ë°˜ë„ì²´ ì¹© 5G í†µì‹  ìš°ì£¼ ë¡œì¼“ ìœ„ì„±", 5),
            ("ì˜í•™ ë°”ì´ì˜¤ ìœ ì „ì ë°±ì‹  ì—°êµ¬ ê°œë°œ", 5),

            # ì„¸ê³„
            ("ë¯¸êµ­ ì¤‘êµ­ ì¼ë³¸ ëŸ¬ì‹œì•„ ìœ ëŸ½ ì˜êµ­", 6),
            ("íŠ¸ëŸ¼í”„ ë°”ì´ë“  ì‹œì§„í•‘ í‘¸í‹´ ì™¸ì‹  í•´ì™¸", 6),
            ("êµ­ì œ ì„¸ê³„ ê¸€ë¡œë²Œ UN NATO ì „ìŸ", 6),
            ("ì™¸êµ ì •ìƒíšŒë‹´ í˜‘ì • ì œì¬ ë‹¬ëŸ¬ ìœ ë¡œ", 6),

            # ìŠ¤í¬ì¸ 
            ("ìŠ¤í¬ì¸  ì¶•êµ¬ ì•¼êµ¬ ë†êµ¬ ë°°êµ¬ ê³¨í”„", 7),
            ("ì„ ìˆ˜ ê°ë… ê²½ê¸° ëŒ€íšŒ ë¦¬ê·¸ ì›”ë“œì»µ", 7),
            ("Kë¦¬ê·¸ KBO ì˜¬ë¦¼í”½ ì†í¥ë¯¼ ë©”ì‹œ", 7),
            ("ë“ì  ê³¨ ìŠ¹ë¦¬ ìš°ìŠ¹ ë©”ë‹¬ ê¸°ë¡", 7),

            # ì—°ì˜ˆ
            ("ì—°ì˜ˆ ì—°ì˜ˆì¸ ê°€ìˆ˜ ë°°ìš° ì•„ì´ëŒ BTS", 7),
            ("ë“œë¼ë§ˆ ì˜í™” ì˜ˆëŠ¥ ë°©ì†¡ ë„·í”Œë¦­ìŠ¤", 8),
            ("ë°ë·” ì»´ë°± ì•¨ë²” ì‹ ê³¡ ì‹œì²­ë¥  í¥í–‰", 8),
            ("ê²°í˜¼ ì—´ì•  ìŠ¤ìº”ë“¤ ì†Œì†ì‚¬ íŒ¬ë¯¸íŒ…", 8),
        ]

        # ë” ë§ì€ í•™ìŠµ ë°ì´í„°ë¥¼ ìœ„í•´ ì¡°í•© ìƒì„±
        extended_samples = []
        for text, label in training_samples:
            extended_samples.append((text, label))
            # ë‹¨ì–´ ìˆœì„œë¥¼ ë°”ê¿”ì„œ ë³€í˜• ë°ì´í„° ì¶”ê°€
            words = text.split()
            if len(words) > 3:
                import random
                random.shuffle(words)
                extended_samples.append((' '.join(words), label))

        schema = StructType([
            StructField("text", StringType(), True),
            StructField("label", IntegerType(), True)
        ])

        return self.spark.createDataFrame(extended_samples, schema)

    def train_model(self):
        """ëª¨ë¸ í•™ìŠµ"""
        print("ğŸ“š í•™ìŠµ ë°ì´í„° ìƒì„± ì¤‘...")
        training_data = self.create_training_data()
        training_data.show(5, truncate=False)

        print("ğŸ”§ ML íŒŒì´í”„ë¼ì¸ ìƒì„± ì¤‘...")

        # 1. Tokenizer: í…ìŠ¤íŠ¸ë¥¼ ë‹¨ì–´ë¡œ ë¶„ë¦¬
        tokenizer = Tokenizer(inputCol="text", outputCol="words")

        # 2. StopWordsRemover: ë¶ˆìš©ì–´ ì œê±°
        remover = StopWordsRemover(inputCol="words", outputCol="filtered_words")
        remover.setStopWords(self.korean_stopwords)

        # 3. HashingTF: ë‹¨ì–´ë¥¼ ìˆ«ì ë²¡í„°ë¡œ ë³€í™˜
        hashingTF = HashingTF(inputCol="filtered_words", outputCol="raw_features", numFeatures=1000)

        # 4. IDF: TF-IDF ê³„ì‚°
        idf = IDF(inputCol="raw_features", outputCol="features")

        # 5. Naive Bayes ë¶„ë¥˜ê¸°
        nb = NaiveBayes(smoothing=1.0, modelType="multinomial")

        # íŒŒì´í”„ë¼ì¸ ìƒì„±
        pipeline = Pipeline(stages=[tokenizer, remover, hashingTF, idf, nb])

        print("ğŸ“ ëª¨ë¸ í•™ìŠµ ì‹œì‘...")
        self.pipeline_model = pipeline.fit(training_data)

        # ëª¨ë¸ ì €ì¥
        print(f"ğŸ’¾ ëª¨ë¸ ì €ì¥ ì¤‘: {self.model_path}")
        os.makedirs(os.path.dirname(self.model_path), exist_ok=True)
        self.pipeline_model.write().overwrite().save(self.model_path)

        print("âœ… ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!")

    def load_model(self):
        """ì €ì¥ëœ ëª¨ë¸ ë¡œë“œ"""
        try:
            if os.path.exists(self.model_path):
                print(f"ğŸ“‚ ëª¨ë¸ ë¡œë”©: {self.model_path}")
                self.pipeline_model = PipelineModel.load(self.model_path)
                print("âœ… ëª¨ë¸ ë¡œë“œ ì„±ê³µ")
                return True
            else:
                print(f"âš ï¸ ëª¨ë¸ íŒŒì¼ ì—†ìŒ: {self.model_path}")
                return False
        except Exception as e:
            print(f"âš ï¸ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False

    def predict(self, title: str, content: str) -> tuple:
        """
        ì¹´í…Œê³ ë¦¬ ì˜ˆì¸¡

        Returns:
            (category_name, category_id, confidence)
        """
        if not self.pipeline_model:
            if not self.load_model():
                print("ğŸ“ ëª¨ë¸ì´ ì—†ì–´ì„œ ìƒˆë¡œ í•™ìŠµí•©ë‹ˆë‹¤...")
                self.train_model()

        # ì œëª©ê³¼ ë³¸ë¬¸ í•©ì¹˜ê¸° (ì œëª© ê°€ì¤‘ì¹˜)
        text = f"{title} {title} {content[:500]}"

        # ë°ì´í„°í”„ë ˆì„ ìƒì„±
        schema = StructType([
            StructField("text", StringType(), True)
        ])
        df = self.spark.createDataFrame([(text,)], schema)

        # ì˜ˆì¸¡
        predictions = self.pipeline_model.transform(df)

        # ê²°ê³¼ ì¶”ì¶œ
        result = predictions.select("prediction", "probability").collect()[0]
        category_id = int(result.prediction)
        probability = result.probability.toArray()
        confidence = float(max(probability))

        category_name = self.categories.get(category_id, 'ê¸°íƒ€')

        print(f"ğŸ¯ ë¶„ë¥˜ ê²°ê³¼: {category_name} (ì‹ ë¢°ë„: {confidence:.2f})")

        return category_name, category_id, confidence


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸
    spark = SparkSession.builder \
        .appName("CategoryClassifier") \
        .master("local[*]") \
        .getOrCreate()

    classifier = CategoryClassifier(spark)

    # ëª¨ë¸ í•™ìŠµ
    classifier.train_model()

    # í…ŒìŠ¤íŠ¸
    test_cases = [
        ("êµ­íšŒ, ìƒˆë¡œìš´ ë²•ì•ˆ í†µê³¼", "êµ­íšŒì—ì„œ ì—¬ì•¼ ì˜ì›ë“¤ì´ ëª¨ì—¬ ìƒˆë¡œìš´ ë²•ì•ˆì„ í†µê³¼ì‹œì¼°ë‹¤."),
        ("ì‚¼ì„±ì „ì, ì‹ ì œí’ˆ ë°œí‘œ", "ì‚¼ì„±ì „ìê°€ ìƒˆë¡œìš´ ìŠ¤ë§ˆíŠ¸í°ì„ ë°œí‘œí–ˆë‹¤. ë°˜ë„ì²´ ê¸°ìˆ ì´ ì ìš©ë˜ì—ˆë‹¤."),
        ("ì†í¥ë¯¼ ê³¨ ë“ì ", "í† íŠ¸ë„˜ ì†í¥ë¯¼ì´ í”„ë¦¬ë¯¸ì–´ë¦¬ê·¸ì—ì„œ ê³¨ì„ ë„£ì—ˆë‹¤."),
    ]

    for title, content in test_cases:
        category, cat_id, confidence = classifier.predict(title, content)
        print(f"ì œëª©: {title}")
        print(f"ê²°ê³¼: {category} (ID: {cat_id}, ì‹ ë¢°ë„: {confidence:.2f})\n")

    spark.stop()

# ===== ì£¼ì„ ì²˜ë¦¬ ë =====
'''
