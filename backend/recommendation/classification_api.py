"""
âš ï¸ ì´ íŒŒì¼ì€ ë” ì´ìƒ ì‚¬ìš©ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤ âš ï¸

Spark ML API â†’ Simple Classifier API(backend/simple-classifier)ë¡œ ëŒ€ì²´ë¨

ì‹¤ì œë¡œ ì´ APIë„ Spark MLì„ ì‚¬ìš©í•˜ì§€ ì•Šê³  ìˆì—ˆìŠµë‹ˆë‹¤ (286ì¤„ ì°¸ì¡°).
ì›ë³¸ ì¹´í…Œê³ ë¦¬ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ëŠ” ë°©ì‹ì´ì—ˆìœ¼ë¯€ë¡œ,
Spark ì˜ì¡´ì„±ë§Œ ì œê±°í•œ ê²½ëŸ‰ ë²„ì „ìœ¼ë¡œ ëŒ€ì²´í–ˆìŠµë‹ˆë‹¤.

ì•„ë˜ ì½”ë“œëŠ” ì°¸ê³ ìš©ìœ¼ë¡œ ì£¼ì„ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤.
í•„ìš” ì‹œ ë³µêµ¬ ê°€ëŠ¥í•˜ë„ë¡ ì½”ë“œë¥¼ ì‚­ì œí•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.
"""

'''
# ===== ì•„ë˜ ì½”ë“œëŠ” ì£¼ì„ ì²˜ë¦¬ë¨ (Spark ëŒ€ì²´) =====

"""
Spark ML ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ API ì„œë¹„ìŠ¤
í¬ë¡¤ëŸ¬ê°€ í˜¸ì¶œí•  ìˆ˜ ìˆëŠ” REST API ì œê³µ
"""

from flask import Flask, request, jsonify
from pyspark.sql import SparkSession
from category_classifier import CategoryClassifier
import logging
import os
import psycopg2
from psycopg2.extras import RealDictCursor
from datetime import datetime

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

app = Flask(__name__)

# Spark ì„¸ì…˜ ì „ì—­ ë³€ìˆ˜
spark = None
classifier = None

def init_spark():
    """Spark ì„¸ì…˜ ì´ˆê¸°í™”"""
    global spark, classifier

    if spark is None:
        logger.info("ğŸš€ Spark ì„¸ì…˜ ì´ˆê¸°í™” ì¤‘...")
        # Use local mode to avoid distributed filesystem issues
        spark = SparkSession.builder \
            .appName("CategoryClassificationAPI") \
            .master("local[*]") \
            .config("spark.driver.memory", "2g") \
            .config("spark.sql.shuffle.partitions", "4") \
            .getOrCreate()

        logger.info("âœ… Spark ì„¸ì…˜ ìƒì„± ì™„ë£Œ")

        # ë¶„ë¥˜ê¸° ì´ˆê¸°í™”
        classifier = CategoryClassifier(spark)

        # ëª¨ë¸ ë¡œë“œ ë˜ëŠ” í•™ìŠµ
        if not classifier.load_model():
            logger.info("ğŸ“ ëª¨ë¸ì´ ì—†ì–´ì„œ í•™ìŠµí•©ë‹ˆë‹¤...")
            classifier.train_model()
        else:
            logger.info("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")

@app.route('/health', methods=['GET'])
def health_check():
    """í—¬ìŠ¤ ì²´í¬"""
    return jsonify({
        'status': 'healthy',
        'service': 'Spark ML Classification API',
        'spark_active': spark is not None
    })

@app.route('/classify', methods=['POST'])
def classify_news():
    """
    ë‰´ìŠ¤ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜

    Request Body:
    {
        "title": "ê¸°ì‚¬ ì œëª©",
        "content": "ê¸°ì‚¬ ë³¸ë¬¸"
    }

    Response:
    {
        "category_name": "ê²½ì œ",
        "category_id": 2,
        "confidence": 0.85
    }
    """
    try:
        data = request.get_json()

        if not data:
            return jsonify({'error': 'No JSON data provided'}), 400

        title = data.get('title', '')
        content = data.get('content', '')

        if not title and not content:
            return jsonify({'error': 'Title or content required'}), 400

        logger.info(f"ğŸ“° ë¶„ë¥˜ ìš”ì²­: {title[:50]}...")

        # ë¶„ë¥˜ ìˆ˜í–‰
        category_name, category_id, confidence = classifier.predict(title, content)

        logger.info(f"âœ… ë¶„ë¥˜ ì™„ë£Œ: {category_name} (ì‹ ë¢°ë„: {confidence:.2f})")

        return jsonify({
            'category_name': category_name,
            'category_id': category_id,
            'confidence': confidence,
            'success': True
        })

    except Exception as e:
        logger.error(f"âŒ ë¶„ë¥˜ ì‹¤íŒ¨: {e}", exc_info=True)
        return jsonify({
            'error': str(e),
            'success': False
        }), 500

@app.route('/batch-classify', methods=['POST'])
def batch_classify():
    """
    ì—¬ëŸ¬ ë‰´ìŠ¤ ë™ì‹œ ë¶„ë¥˜

    Request Body:
    {
        "articles": [
            {"title": "ì œëª©1", "content": "ë³¸ë¬¸1"},
            {"title": "ì œëª©2", "content": "ë³¸ë¬¸2"}
        ]
    }
    """
    try:
        data = request.get_json()
        articles = data.get('articles', [])

        if not articles:
            return jsonify({'error': 'No articles provided'}), 400

        logger.info(f"ğŸ“š ë°°ì¹˜ ë¶„ë¥˜ ìš”ì²­: {len(articles)}ê°œ ê¸°ì‚¬")

        results = []
        for article in articles:
            title = article.get('title', '')
            content = article.get('content', '')

            try:
                category_name, category_id, confidence = classifier.predict(title, content)
                results.append({
                    'category_name': category_name,
                    'category_id': category_id,
                    'confidence': confidence,
                    'success': True
                })
            except Exception as e:
                logger.error(f"âŒ ê¸°ì‚¬ ë¶„ë¥˜ ì‹¤íŒ¨: {e}")
                results.append({
                    'error': str(e),
                    'success': False
                })

        logger.info(f"âœ… ë°°ì¹˜ ë¶„ë¥˜ ì™„ë£Œ: {len(results)}ê°œ")

        return jsonify({
            'results': results,
            'total': len(results),
            'success': True
        })

    except Exception as e:
        logger.error(f"âŒ ë°°ì¹˜ ë¶„ë¥˜ ì‹¤íŒ¨: {e}", exc_info=True)
        return jsonify({
            'error': str(e),
            'success': False
        }), 500

@app.route('/retrain', methods=['POST'])
def retrain_model():
    """ëª¨ë¸ ì¬í•™ìŠµ"""
    try:
        logger.info("ğŸ“ ëª¨ë¸ ì¬í•™ìŠµ ì‹œì‘...")
        classifier.train_model()
        logger.info("âœ… ëª¨ë¸ ì¬í•™ìŠµ ì™„ë£Œ")

        return jsonify({
            'message': 'Model retrained successfully',
            'success': True
        })
    except Exception as e:
        logger.error(f"âŒ ì¬í•™ìŠµ ì‹¤íŒ¨: {e}", exc_info=True)
        return jsonify({
            'error': str(e),
            'success': False
        }), 500

def get_db_connection():
    """PostgreSQL ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°"""
    return psycopg2.connect(
        host=os.getenv('DB_HOST', 'postgres'),
        port=int(os.getenv('DB_PORT', 5432)),
        user=os.getenv('POSTGRES_USER', 'fans_user'),
        password=os.getenv('POSTGRES_PASSWORD', 'fans_password'),
        database=os.getenv('POSTGRES_DB', 'fans_db')
    )

# ì–¸ë¡ ì‚¬ ë§¤í•‘
SOURCE_MAP = {
    'ì—°í•©ë‰´ìŠ¤': 1, 'ë™ì•„ì¼ë³´': 20, 'ë¬¸í™”ì¼ë³´': 21,
    'ì„¸ê³„ì¼ë³´': 22, 'ì¡°ì„ ì¼ë³´': 23, 'ì¤‘ì•™ì¼ë³´': 25,
    'í•œê²¨ë ˆ': 28, 'ê²½í–¥ì‹ ë¬¸': 32, 'í•œêµ­ì¼ë³´': 55,
    'ë§¤ì¼ê²½ì œ': 56, 'í•œêµ­ê²½ì œ': 214, 'ë¨¸ë‹ˆíˆ¬ë°ì´': 421,
    'YTN': 437, 'JTBC': 448,
    'ê¸°íƒ€': 449
}

def classify_source(original_source):
    """ì–¸ë¡ ì‚¬ í…ìŠ¤íŠ¸ë¥¼ source_idë¡œ ë§¤í•‘"""
    if not original_source:
        return 449  # ê¸°íƒ€

    # ì™„ì „ ì¼ì¹˜ ê²€ìƒ‰
    if original_source in SOURCE_MAP:
        return SOURCE_MAP[original_source]

    # ë¶€ë¶„ ì¼ì¹˜ ê²€ìƒ‰
    for source_name, source_id in SOURCE_MAP.items():
        if source_name in original_source or original_source in source_name:
            return source_id

    return 449  # ê¸°íƒ€

@app.route('/process-raw-news', methods=['POST'])
def process_raw_news():
    """
    raw_news_articlesì—ì„œ ë¯¸ì²˜ë¦¬ ê¸°ì‚¬ë¥¼ ì½ì–´ì„œ ë¶„ë¥˜í•˜ê³  news_articlesë¡œ ì´ë™

    Request Body (ì„ íƒì ):
    {
        "limit": 100  # í•œ ë²ˆì— ì²˜ë¦¬í•  ê¸°ì‚¬ ìˆ˜ (ê¸°ë³¸ê°’: 50)
    }
    """
    try:
        data = request.get_json() or {}
        limit = data.get('limit', 50)

        logger.info(f"ğŸ”„ ì›ë³¸ ê¸°ì‚¬ ì²˜ë¦¬ ì‹œì‘ (ìµœëŒ€ {limit}ê°œ)...")

        conn = get_db_connection()
        cursor = conn.cursor(cursor_factory=RealDictCursor)

        # ë¯¸ì²˜ë¦¬ raw ê¸°ì‚¬ ì¡°íšŒ
        cursor.execute("""
            SELECT id, title, content, url, image_url, journalist, pub_date,
                   original_source, original_category
            FROM raw_news_articles
            WHERE processed = FALSE
            ORDER BY created_at ASC
            LIMIT %s
        """, (limit,))

        raw_articles = cursor.fetchall()

        if not raw_articles:
            logger.info("âœ… ì²˜ë¦¬í•  ì›ë³¸ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤")
            cursor.close()
            conn.close()
            return jsonify({
                'message': 'No raw articles to process',
                'processed': 0,
                'success': True
            })

        logger.info(f"ğŸ“š {len(raw_articles)}ê°œ ì›ë³¸ ê¸°ì‚¬ ì²˜ë¦¬ ì¤‘...")

        processed_count = 0
        failed_count = 0

        # ì¹´í…Œê³ ë¦¬ ID ë§¤í•‘
        category_map = {
            'ì •ì¹˜': 1,
            'ê²½ì œ': 2,
            'ì‚¬íšŒ': 3,
            'ìƒí™œ/ë¬¸í™”': 4,
            'IT/ê³¼í•™': 5,
            'ì„¸ê³„': 6,
            'ìŠ¤í¬ì¸ ': 7,
            'ì—°ì˜ˆ': 8
        }

        for raw_article in raw_articles:
            try:
                # Spark ML ì‚¬ìš© ì•ˆí•¨! ê¸°ì‚¬ í˜ì´ì§€ì—ì„œ íŒŒì‹±í•œ ì›ë³¸ ì¹´í…Œê³ ë¦¬ ì‚¬ìš©
                original_category = raw_article['original_category'] or 'ì‚¬íšŒ'
                category_id = category_map.get(original_category, 3)  # ê¸°ë³¸ê°’: ì‚¬íšŒ(3)

                # ì–¸ë¡ ì‚¬ ë¶„ë¥˜
                source_id = classify_source(raw_article['original_source'])

                logger.info(f"âœ… ì¹´í…Œê³ ë¦¬ í™•ì •: {raw_article['title'][:50]}... -> {original_category} (ì–¸ë¡ ì‚¬: {source_id})")

                # news_articlesì— ì‚½ì…
                cursor.execute("""
                    INSERT INTO news_articles
                    (title, content, url, image_url, journalist, pub_date, source_id, category_id)
                    VALUES (%s, %s, %s, %s, %s, %s, %s, %s)
                    ON CONFLICT (url) DO NOTHING
                """, (
                    raw_article['title'],
                    raw_article['content'],
                    raw_article['url'],
                    raw_article['image_url'],
                    raw_article['journalist'],
                    raw_article['pub_date'],
                    source_id,
                    category_id
                ))

                # raw_news_articles ìƒíƒœ ì—…ë°ì´íŠ¸
                cursor.execute("""
                    UPDATE raw_news_articles
                    SET processed = TRUE, processed_at = %s
                    WHERE id = %s
                """, (datetime.now(), raw_article['id']))

                processed_count += 1

            except Exception as e:
                logger.error(f"âŒ ê¸°ì‚¬ ì²˜ë¦¬ ì‹¤íŒ¨ (ID: {raw_article['id']}): {e}")

                # ì—ëŸ¬ ê¸°ë¡
                cursor.execute("""
                    UPDATE raw_news_articles
                    SET processing_error = %s
                    WHERE id = %s
                """, (str(e), raw_article['id']))

                failed_count += 1

        conn.commit()
        cursor.close()
        conn.close()

        logger.info(f"âœ… ì²˜ë¦¬ ì™„ë£Œ: {processed_count}ê°œ ì„±ê³µ, {failed_count}ê°œ ì‹¤íŒ¨")

        return jsonify({
            'message': f'Processed {processed_count} articles successfully',
            'processed': processed_count,
            'failed': failed_count,
            'total': len(raw_articles),
            'success': True
        })

    except Exception as e:
        logger.error(f"âŒ ì›ë³¸ ê¸°ì‚¬ ì²˜ë¦¬ ì‹¤íŒ¨: {e}", exc_info=True)
        return jsonify({
            'error': str(e),
            'success': False
        }), 500

if __name__ == '__main__':
    # Spark ì´ˆê¸°í™”
    init_spark()

    # Flask ì„œë²„ ì‹œì‘
    port = int(os.getenv('CLASSIFICATION_API_PORT', 5000))
    logger.info(f"ğŸŒ Classification API ì„œë²„ ì‹œì‘: í¬íŠ¸ {port}")
    app.run(host='0.0.0.0', port=port, debug=False)

# ===== ì£¼ì„ ì²˜ë¦¬ ë =====
'''
