# FANS 프로젝트 작업 로그

## 2025-09-24 작업 내용

### 🎯 주요 작업: 크롤링 서비스 마이크로서비스 분리 및 이중화 구현

#### 1. 백엔드에서 크롤러 서비스 분리
**배경:**
- 기존에는 백엔드 API 서버와 크롤링 로직이 하나로 통합되어 있었음
- 크롤링 문제 발생시 전체 백엔드 서비스에 영향
- 개발/디버깅시 전체 시스템을 재시작해야 하는 불편함

**작업 내용:**
- `backend/crawler` 디렉토리로 크롤링 서비스 완전 분리
- 독립적인 Express.js 서비스로 구현 (포트 4001)
- 기존 크롤링 로직을 `newsCrawlerService.ts`에서 이전
- 데이터베이스 연결 및 엔티티 최소화 (불필요한 의존성 제거)

**결과:**
- 크롤링 서비스 독립 실행 가능
- 백엔드 API와 분리된 개발/배포 가능
- 포트 충돌 문제 해결 (PORT → CRAWLER_PORT)

#### 2. 크롤링 효율성 문제 발견 및 분석
**문제 상황:**
- 예상 수집량: 24개 (8개 카테고리 × 3사이클)
- 실제 수집량: 4개 (16.7% 성공률)
- 모든 언론사가 "연합뉴스"로 표시되는 문제
- 기자 정보가 "없음"으로 나타나는 문제

**원인 분석:**
- Naver API가 주로 지역/소규모 언론사 결과 반환
- 타겟 언론사 필터링으로 인해 대부분 뉴스 차단됨
- 14개 주요 언론사만 허용하는 필터가 과도하게 작동

**해결 방향:**
- RSS 피드를 통한 직접 수집 방식 도입 검토

#### 3. RSS 크롤링 시스템 구현
**동기:**
- Naver API의 한계를 보완하기 위한 대안 필요
- 주요 언론사의 RSS 피드 직접 수집으로 품질 향상
- API와 RSS 이중화를 통한 안정성 확보

**구현 내용:**
```
- 새로운 파일: `rssCrawlerService.ts`
- 대상 언론사: 5개 (경향신문, 동아일보, 한겨레, 조선일보, 국민일보)
- RSS 파싱: xml2js 라이브러리 활용
- 각 언론사당 10개 뉴스 수집 설정
```

**기술적 구현:**
- RSS XML을 JSON으로 파싱
- 기존 데이터베이스 저장 로직 재사용
- 중복 검사 로직 (URL 기준)
- 기자 이름 추출 및 정리 로직 적용

#### 4. 이중 크롤링 시스템 설계
**아키텍처 변경:**
- 기존: 카테고리 기반 2x2 구조 계획
- 변경: **API/RSS 기반 2x2 구조**

**새로운 구조:**
```
Group A: API 크롤링
├── API Crawler A1 (Primary, Port 4001)
└── API Crawler A2 (Secondary, Port 4003)

Group B: RSS 크롤링
├── RSS Crawler B1 (Primary, Port 4002)
└── RSS Crawler B2 (Secondary, Port 4004)
```

**엔드포인트 구현:**
- `POST /crawl/api/start`: API만 크롤링
- `POST /crawl/rss/start`: RSS만 크롤링
- `POST /crawl/all/start`: 통합 크롤링 (API + RSS)

#### 5. RSS 피드 호환성 검증
**테스트 결과:**
- ✅ 경향신문: 완전한 XML 구조
- ✅ 동아일보: 이미지 포함된 풍부한 데이터
- ✅ 한겨레신문: 정상 RSS 응답
- ✅ 조선일보: 다양한 메타데이터 포함
- ✅ 국민일보: 정상 응답
- ❌ 뉴시스: 301 리다이렉트 (URL 변경 필요)

#### 6. 기자 이름 추출 로직 개선
**개선 사항:**
- 이메일 주소 제거: `[가-힣]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}`
- 직책명 제거: "기자", "특파원", "편집위원", "논설위원"
- 괄호/대괄호 내용 제거
- 특수문자 정리 및 공백 정규화

#### 7. 패키지 의존성 관리
**새로 추가된 패키지:**
- `xml2js`: RSS XML 파싱
- `@types/xml2js`: TypeScript 타입 정의
- `@types/cors`: CORS TypeScript 타입

### 📊 예상 성능 개선 효과

**수집량 증가:**
- 기존: 8개 (실제 4개만 저장)
- 예상: RSS 50개 + API 8개 = 58개

**품질 향상:**
- RSS: 정확한 출처/기자명 확보
- 다양한 이미지 및 메타데이터 포함

**안정성 향상:**
- API 장애시 RSS 백업
- RSS 장애시 API 백업
- 독립적 서비스 운영

### 🔧 기술적 변경사항

**코드 구조:**
- API 크롤러와 RSS 크롤러 완전 분리
- 공통 데이터베이스 저장 로직 재사용
- 중복 검사 로직 통합

**성능 최적화:**
- RSS 피드간 1초 딜레이로 서버 부하 분산
- 타임아웃 10초 설정
- 중복 URL 기반 중복 제거

### 📋 다음 단계

**즉시 진행 예정:**
- [ ] RSS 크롤러 실제 운영 테스트
- [ ] 저장 개수 기준 크롤링 로직 수정
- [ ] Docker 컨테이너화

**향후 계획:**
- [ ] 4개 인스턴스 Active-Active 클러스터 구축
- [ ] 헬스체크 및 자동 페일오버 시스템
- [ ] 모니터링 및 알림 시스템

### 🎯 핵심 성과

1. **마이크로서비스 분리 완료**: 크롤링 서비스 독립 운영 가능
2. **이중화 시스템 구축**: API/RSS 상호 보완적 크롤링
3. **품질 향상 기반 마련**: RSS 직접 수집으로 정확도 확보
4. **확장성 확보**: 각 크롤러 독립적 확장 가능
5. **개발 효율성 향상**: 분리된 서비스로 빠른 개발/테스트

---

**작업일**: 2025-09-24
**소요시간**: 약 8시간
**주요 기여자**: 개발자 
**문서 업데이트**: `CRAWLER_DEVELOPMENT_LOG.md`, `CRAWLER_SERVICE_SEPARATION_PLAN.md`